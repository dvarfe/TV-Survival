{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d21fe79a",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Data preprocessing steps for model training.\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. **Download data** \n",
    "2. **Convert to Parquet** - `datastats2parquet.py` \n",
    "3. **Truncate data** - `truncate.py`  \n",
    "4. **Split data** - `Splitter.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7470e63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data directory: ../RawData\n",
      "Processed data directory: ../Data\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "raw_data_dir = Path('../RawData')\n",
    "data_dir = Path('../Data')\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "raw_data_dir.mkdir(exist_ok=True)\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Raw data directory: {raw_data_dir}\")\n",
    "print(f\"Processed data directory: {data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4789a160",
   "metadata": {},
   "source": [
    "## Step 1: Download Dataset\n",
    "\n",
    "In this step, we download the raw dataset.\n",
    "The following cells will download the required dataset files using wget commands. Make sure you have sufficient disk space available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e466b158",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../RawData\n",
    "\n",
    "# Download commands\n",
    "wget https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_2015.zip\n",
    "# wget https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q1_2016.zip\n",
    "# wget https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q2_2016.zip\n",
    "# wget https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q3_2016.zip\n",
    "# wget https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q4_2016.zip\n",
    "# wget https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q1_2017.zip\n",
    "# wget https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q2_2017.zip\n",
    "# wget https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q3_2017.zip\n",
    "wget https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q4_2017.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1425e3",
   "metadata": {},
   "source": [
    "## Step 2: Convert Data Statistics to Parquet\n",
    "\n",
    "The `datastats2parquet.py` script converts raw CSV data files into Parquet format. This step unzips archives and concatenates data. Set `unique_ids` higher than the actual number of drives to ensure all drives are included in the final file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9be5bc2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatastats2parquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m main \u001b[38;5;28;01mas\u001b[39;00m unzip_and_agg\n\u001b[1;32m      3\u001b[0m unzip_and_agg(data_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, unique_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m, frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, sample_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2016.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/TV-Survival/demo/../src/preprocessing/datastats2parquet.py:28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rmtree\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "from preprocessing.datastats2parquet import main as unzip_and_agg\n",
    "\n",
    "unzip_and_agg(data_folder='.', unique_ids=20000, frequency=1, sample_file='2016.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faf6337",
   "metadata": {},
   "source": [
    "## Step 3: Truncate Data\n",
    "\n",
    "The `truncate.py` script processes the converted Parquet data to handle truncation. Truncated drives are those that are censored only because the observation period ended (right-censored observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7d9ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.truncate import truncate_observations\n",
    "\n",
    "truncate_observations(input_file='merged.parquet', output_file='2016_2018_trunc.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857c10e",
   "metadata": {},
   "source": [
    "## Step 4: Split Dataset\n",
    "\n",
    "The `Splitter.py` script performs the final preprocessing step by applying the complete preprocessing pipeline and splitting the data:\n",
    "\n",
    "- **Stratified sampling**: Creates balanced train/test splits preserving failure rate distribution (train and test sets do not overlap by drives)\n",
    "- **Feature engineering**: Applies time transformations, aggregation, and scaling\n",
    "- **Multiple sample sizes**: Generates datasets with different numbers of observations per drive\n",
    "- **Quality control**: Removes drives with insufficient data or anomalous patterns\n",
    "\n",
    "This step produces the final preprocessed datasets ready for survival model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c568c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.Splitter import main as split_data\n",
    "\n",
    "split_data(input_file='2016_2018_trunc.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7bee22",
   "metadata": {},
   "source": [
    "## Done\n",
    "\n",
    "Preprocessing completed. Generated files:\n",
    "- `{sample_size}_train_preprocessed.csv` - training data\n",
    "- `{sample_size}_{test_size}_test_preprocessed.csv` - test data\n",
    "\n",
    "Next: use `training_demo.ipynb` for model training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf23eb19",
   "metadata": {},
   "source": [
    "# Aggregation of Pre-trained Models\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Complete model training in `training_demo.ipynb`\n",
    "2. Load pre-trained models from experiments\n",
    "3. Configure aggregation methods and weights\n",
    "4. Run aggregation experiments with different sample sizes\n",
    "5. Evaluate aggregated predictions using survival metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052a4c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append('../src')\n",
    "from aggregation.PredictionsAggregator import PredictionsAggregator\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea06206d",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Setup aggregation experiment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c09935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "DATASET_TRAIN_SAMPLES = 20 # Number of samples in dataset which will be used as a train dataset\n",
    "MODEL_TRAIN_SAMPLES = 20 # Number of samples the model was trained on (for model selection)\n",
    "SAMPLE_GRID = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Number of samples for aggregation\n",
    "TEST_SAMPLES = [25]\n",
    "MODELS_SIZE = [2048] # Hidden size of the model (for model selection)\n",
    "MODEL_NAME = 'CoxTV'\n",
    "METRICS_LIST = {'ci', 'ibs'}\n",
    "DATA_EXT = 'csv'\n",
    "\n",
    "DATA_FOLDER = Path(\"../Data/Preprocessed_new\") # Path to preprocessed data\n",
    "RES_FOLDER = Path(f\"../Data/Agg\")\n",
    "MODELS_FOLDER = Path(f\"demo_models\") # Path to trained models for aggregation demo\n",
    "\n",
    "TIMES = np.arange(0, 730)\n",
    "TRAIN_BATCHSIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd84ab9e",
   "metadata": {},
   "source": [
    "## Aggregation Methods\n",
    "\n",
    "Define different prediction aggregation strategies with various weight distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define aggregation methods with different weights using real PredictionsAggregator\n",
    "AGGREGATORS_DICT = {\n",
    "    \"n_dist\": {\n",
    "        \"0.01\": PredictionsAggregator(mode='n_dist', weight=0.01),\n",
    "        \"0.1\": PredictionsAggregator(mode='n_dist', weight=0.1),\n",
    "        \"0.3\": PredictionsAggregator(mode='n_dist', weight=0.3),\n",
    "        \"0.5\": PredictionsAggregator(mode='n_dist', weight=0.5),\n",
    "        \"0.7\": PredictionsAggregator(mode='n_dist', weight=0.7),\n",
    "        \"0.9\": PredictionsAggregator(mode='n_dist', weight=0.9),\n",
    "        \"0.99\": PredictionsAggregator(mode='n_dist', weight=0.99)\n",
    "    },\n",
    "    \"t_dist\": {\n",
    "        \"0.1\": PredictionsAggregator(mode='t_dist', weight=0.1),\n",
    "        \"1\": PredictionsAggregator(mode='t_dist', weight=1),\n",
    "        \"10\": PredictionsAggregator(mode='t_dist', weight=10),\n",
    "        \"25\": PredictionsAggregator(mode='t_dist', weight=25),\n",
    "        \"50\": PredictionsAggregator(mode='t_dist', weight=50),\n",
    "        \"100\": PredictionsAggregator(mode='t_dist', weight=100),\n",
    "        \"1000\": PredictionsAggregator(mode='t_dist', weight=1000)\n",
    "    },\n",
    "    \"geom\": {\n",
    "        \"0.01\": PredictionsAggregator(mode='geom', weight=0.01),\n",
    "        \"0.1\": PredictionsAggregator(mode='geom', weight=0.1),\n",
    "        \"0.3\": PredictionsAggregator(mode='geom', weight=0.3),\n",
    "        \"0.5\": PredictionsAggregator(mode='geom', weight=0.5),\n",
    "        \"0.7\": PredictionsAggregator(mode='geom', weight=0.7),\n",
    "        \"0.9\": PredictionsAggregator(mode='geom', weight=0.9),\n",
    "        \"0.99\": PredictionsAggregator(mode='geom', weight=0.99)\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Aggregation methods configured:\")\n",
    "for method, variants in AGGREGATORS_DICT.items():\n",
    "    print(f\"- {method}: {list(variants.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a3dfe",
   "metadata": {},
   "source": [
    "## Functions from Agg.py\n",
    "\n",
    "Import and setup key functions for aggregation experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128c27d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src/train_scripts')\n",
    "\n",
    "# Import functions for aggregation experiments  \n",
    "from train_scripts.Agg import eval_model\n",
    "\n",
    "# Import ModelScorer for evaluation\n",
    "from scoring.ModelScorer import ModelScorer\n",
    "from utils.Dataset import DiskDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_demo_schema(metrics_list):\n",
    "    \"\"\"Create schema for demo aggregation results\"\"\"\n",
    "    base_schema = {\n",
    "        'train_samples': [],\n",
    "        'agg_samples': [],\n",
    "        'method': [],\n",
    "        'agg_method': [],\n",
    "        'agg_weight': [],\n",
    "        'model_id': []\n",
    "    }\n",
    "    \n",
    "    schema = copy.deepcopy(base_schema)\n",
    "    for metric in metrics_list:\n",
    "        schema[f'{metric}_train'] = []\n",
    "        schema[f'{metric}_test'] = []\n",
    "    \n",
    "    return schema\n",
    "\n",
    "def save_results_to_csv(results_df, filename):\n",
    "    \"\"\"Save aggregation results to CSV file\"\"\"\n",
    "    results_df.to_csv(filename, index=False)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5833933f",
   "metadata": {},
   "source": [
    "## Model Loading and Data Preparation\n",
    "\n",
    "Load pre-trained models and prepare test data for aggregation experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for demo results\n",
    "RES_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create demo results schema\n",
    "SCHEMA = create_demo_schema(METRICS_LIST)\n",
    "results_filename = RES_FOLDER / f\"aggregation_results_{DATASET_TRAIN_SAMPLES}_{max(SAMPLE_GRID)}.csv\"\n",
    "\n",
    "# Look for trained models from training demo\n",
    "model_files = list(MODELS_FOLDER.glob(\"*.pkl\"))\n",
    "if not model_files:\n",
    "    raise FileNotFoundError(f\"No trained models found in {MODELS_FOLDER}. Please run training_demo.ipynb first.\")\n",
    "\n",
    "print(f\"Found {len(model_files)} trained models:\")\n",
    "for model_file in model_files:\n",
    "    print(f\"  - {model_file.name}\")\n",
    "\n",
    "# Create model_name_grid from all available models\n",
    "model_name_grid = [model_file.stem for model_file in model_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad7330e",
   "metadata": {},
   "source": [
    "## Run Aggregation Experiments\n",
    "\n",
    "Execute aggregation experiments with different methods and sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aac9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data path\n",
    "test_data_path = DATA_FOLDER / f\"{DATASET_TRAIN_SAMPLES}_{TEST_SAMPLES[0]}_test_preprocessed.{DATA_EXT}\"\n",
    "\n",
    "# Run evaluation for each model and collect results\n",
    "all_results = []\n",
    "\n",
    "for i, model_name in enumerate(model_name_grid, 1):\n",
    "    print(f\"Processing model {i}/{len(model_name_grid)}: {model_name}\")\n",
    "    try:\n",
    "        model_results = eval_model(\n",
    "            data_path=str(test_data_path),\n",
    "            model_name=model_name,\n",
    "            agg_dict=AGGREGATORS_DICT,\n",
    "            metrics_list=METRICS_LIST,\n",
    "            metric_postfix='test'\n",
    "        )\n",
    "        \n",
    "        print(f\"Model {model_name} completed: {len(model_results)} results\")\n",
    "        all_results.append(model_results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing model {model_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Combine all results into one dataframe\n",
    "if all_results:\n",
    "    results_df = pd.concat(all_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1dafd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
